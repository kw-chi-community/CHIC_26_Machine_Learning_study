## activation function (활성화 함수)

(1) 용어

- 입력(input) x : 데이터 입력 값
- 가중치(weight) w : 각 입력이 얼마나 중요한지 정하는 “중요도 다이얼”
- 편향(bias) b : 전체 기준을 왼쪽/오른쪽으로 밀어주는 “기본값(문턱 조절)”

(2) 가중합

가중합 : $z(x) = b + w^T x$

$w^T x$ : 가중치랑 입력을 각각 곱해서 전부 더한 것

→ 선형(직선) 모델이므로, 꺾이거나 휘는 모양을 만들지 못하기 때문에 XOR 같은 선형분리 불가능 문제를 해결하지 못함

⇒ activation function(활성화 함수) σ(시그마)를 적용 시키기

(3) 활성화 함수

가중합 z를 입력으로 받아서, 뉴런의 반응을 비선형적으로 바꿔주는 함수

- y = σ(z(x))
    
    “선형 결합(직선) + 비선형 변환(활성화)” 
    
- activation 관점
    - 초기 퍼셉트론 관점
        
        threshold(계단 함수, Heaviside step)로 “z가 임계값 넘으면 1, 아니면 0”처럼 딱 잘라서 분류 결정을 내리는 규칙
        
    - 신경망(은닉층) 관점
        
        Sigmoid, Tanh, ReLU 사용
        
    

---

## 활성화 함수의 종류

### sigmoid (시그모이드)

(1) 정의

sigmoid는 입력값 z를 0~1 사이로 출력되도록 하는 함수

$σ(z) = 1 / (1 + exp(-z))$

- z가 아주 크면 → 출력은 1에 가까워짐
- z가 아주 작으면 → 출력은 0에 가까워짐
- z가 0이면 → 출력은 정확히 0.5

(2) 장점

- 출력이 0~1이라 “확률처럼” 해석 가능
    
    → 이진분류에서 자주 등장해.
    
- 매끄러운 곡선이라서 미분이 가능
    
    학습에서 가중치를 업데이트할 때 “손실이 w에 대해 얼마나 민감한지(기울기)”가 필요
    
    미분이 불가능한 구간이 많다면 학습이 불리
    
    시그모이드는 미분결과가 자기 자신(σ)으로 표현
    
    → 계산이 단순하고 안정적
    

(3) 단점

- sigmoid 출력이 0에 매우 가깝거나 1에 매우 가까울 경우
    
    $σ(z)(1 − σ(z)) ≈ 0$
    
    가중치를 바꿔도 출력이 거의 바뀌지 않음 : gradient vanishing(기울기 소실)
    
    → 학습이 매우 느려지거나 멈춘 것처럼 보이게 됨
    

### ReLU

(1) 정의

ReLU는 z가 음수면 0을 내보내고, 양수면 그대로 통과시키는 함수

$ReLU(z) = max(0, z)$

- z가 음수면 → 0
- z가 양수면 → z 그대로

(2) 장점

- 양수 구간에서 계산이 단순
    
    ReLU의 미분
    
    - z < 0 → 미분 0
    - z > 0 → 미분 1
    
    양수 구간에서는 가중치를 바꾸어도 sigmoid처럼 기울기 소실 현상이 나타나지 않음
    
    → 학습이 빠르고 깊은 모델에서도 학습이 잘 됨
    

(3) 단점

- z가 항상 음수인 경우
    
    출력은 계속 0, 미분도 0
    
    → 영원히 학습이 안 됨 (죽은 Relu)
    

---

## ShallowNN (얕은 신경망)

(1) 정의

은닉층(hidden layer)이 **1개인 신경망**

입력층 → 은닉층 1개 → 출력층 구조

(2) 구조

- 입력 x
- 가중합 + 활성화
- 출력 y

```
Input → Hidden(1개) → Output
```

(3) 특징

- 구현이 간단, 계산량이 적음
- XOR 같은 간단한 비선형 문제는 해결 가능하지만, 이미지·음성처럼 복잡한 데이터에는 부족

---

## DeepNN (심층 신경망, DNN)

(1) 정의

은닉층이 **2개 이상인 신경망,** 여러 층을 거치며 점점 추상적인 특징을 학습

(2) 구조

```
Input → Hidden1 → Hidden2 → Hidden3 → ... → Output
```

층이 깊어질수록 단순 특징 → 복잡한 특징 학습

(3) 특징

- 복잡한 데이터 학습 가능, 표현력이 강함
- 기울기 소실 문제 발생 가능 → ReLU 같은 활성화 함수 사용

---

# AE (AutoEncoder)

(1) 정의

입력을 다시 자기 자신으로 복원하는 신경망

목적: **데이터 압축 + 특징 추출**

(2) 구조

```
Input → Encoder → Latent(압축) → Decoder → Output(복원)
```

- Encoder : 정보를 압축
- Decoder : 압축된 정보를 다시 복원

(3) 특징

- 차원 축소 가능 (PCA와 유사 목적), 이상치 탐지에 사용, 노이즈 제거에 사용 (Denoising AE)

---

# RNN (순환 신경망)

(1) 정의

**이전 정보를 기억하면서** 다음 계산을 하는 신경망(시계열 데이터 처리용)

(2) 구조 개념

현재 입력 + 이전 은닉상태 → 새로운 은닉상태

```
h_t = f(x_t, h_{t-1})
```

(과거 정보가 다음 계산에 영향을 줌)

(3) 특징

- 문장, 음성, 주가 등 순서가 중요한 데이터에 사용
- 하지만 긴 문장에서 기울기 소실, 기울기 폭주 발생 → 오래된 정보 기억 어려움

---

# LSTM

(1) 정의

RNN의 기울기 소실 문제를 해결한 모델 (Long Short-Term Memory)

(2) 핵심 아이디어

**Cell State(기억 통로)를 따로 둔다**

- Forget Gate (버릴 정보)
- Input Gate (저장할 정보)
- Output Gate (내보낼 정보)

(3) 특징

- 긴 문장에서도 정보 유지 가능, 번역, 음성인식에 자주 사용
- 구조가 복잡 → 계산량 증가

---

## Seq2Seq

(1) 정의

입력 시퀀스를 다른 시퀀스로 변환하는 구조 (ex: 번역)

(2) 구조

```
Encoder → Context Vector → Decoder
```

- Encoder: 입력 문장을 압축
- Decoder: 출력 문장 생성

예)"나는 학생이다"→ "I am a student"

(3) 특징

- 번역, 요약, 챗봇에 사용
- 긴 문장에서 정보 압축 한계 존재
- 이후 Attention, Transformer로 발전
