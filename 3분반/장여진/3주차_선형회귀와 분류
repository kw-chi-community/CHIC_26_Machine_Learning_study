## 1. 문제 설정과 데이터 구조

### 문제 설정

- **X**: 원인, 입력값 (독립변수)
- **Y**: 결과, 출력값 (종속변수)

머신러닝 → **여러 변수 X로 하나의 결과 Y를 예측**하는 것

### 데이터 구조

- 데이터 = **샘플 × 변수**
- **Sample(샘플)**: 하나의 관측값 (한 줄)
- **Variable(변수)**: 특징/피처 (한 열)
    - 단변량: 변수 1개
    - 다변량: 변수 2개 이상

---

## 2. 예측 문제와 선형회귀 모델

### 예측 문제의 종류

- 수치 예측 → **회귀(Regression)**
- 범주 예측 → **분류(Classification)**

### 선형회귀 모델 정의

- 선형회귀 =  **연속적인 숫자 Y**를 예측하는 모델
- 단순 선형회귀:
    - `Y = β₀ + β₁X`
    - β₀: Y절편(시작점)
    - β₁: 기울기(X가 1 증가할 때 Y 변화량)
    

### 선형 결합(다변수 확장)

- `Y = β₀ + β₁X₁ + β₂X₂ + ...`
- 상수배와 덧셈으로 이루어진 구조 → **선형결합**

### 예측값과 오차의 직관

- 모델이 실제로 내는 출력:
    - `ŷ = β₀ + β₁x`
        
        (다변수: `ŷ = β₀ + β₁x₁ + ... + βₚxₚ`)
        
- 점과 회귀선 사이의 거리 = **오차**
- 회귀선에 가까울수록 예측이 잘 된 것

---

## 3. 선형회귀의 확률적 해석 (오차, 가정, 평균)

### 확정적 관계 vs 확률적 관계

- 확정적 관계:
    - `Y = f(X)` (오차 없음)
- 확률적 관계(현실):
    - `Y = f(X) + ε`
    - ε: 설명되지 않는 오차

결론

- 현실 데이터는 X 하나만으로 Y를 완벽히 설명하기 어려움
    
    ⇒ 선형회귀는 **확률적 모델**로 해석
    

### 오차항에 대한 가정

- ε의 평균 = 0
- 분산 = σ²
- 정규분포를 따른다
    
    → 이 가정이 있어야 수학적으로 모델 추정이 가능
    

### 평균(기대값)의 의미

- `E(Y) = β₀ + β₁X`
- 회귀선 = Y의 평균적 경향(기대값)을 나타내는 선
    
    → 개별 점 예측과 평균 경향은 구분해서 이해
    

---

## 4. 비용함수와 최소제곱 기반 학습(OLS)

### 비용함수가 필요한 이유

- 선형회귀는 **모델 형태만** 주어짐
    - `ŷ = β₀ + β₁x`
- β₀, β₁ 값은 **데이터에 맞게 학습으로 결정**
- 좋은 모델의 기준:
    - 데이터 점들과 회귀선 사이의 **오차가 작은 모델**

### 비용함수(Loss Function)

- 예측이 얼마나 틀렸는지를 수치화
- 선형회귀의 비용함수(제곱오차합):
    - `Loss = Σ (yᵢ − ŷᵢ)²`
    - `Loss = Σ (yᵢ − (β₀ + β₁xᵢ))²`

### 학습 목표

- `argmin_{β₀, β₁} Σ (yᵢ − (β₀ + β₁xᵢ))²`

---

## 5. 비용함수의 성질과 해의 존재 (Convex, Closed-form)

### 비용함수의 성질

- 선형회귀 비용함수는 β₀, β₁에 대해 **2차함수**
- 그래프는 그릇 모양 → **Convex**

Convex의 의미

- 전역 최적해(Global minimum)가 1개
- 학습이 안정적이고 해가 명확

Non-convex와 비교

- Non-convex는 지역 최적해에 빠질 수 있음
- 선형회귀는 그런 위험이 거의 없음

### 최소제곱 해(Closed-form solution)

- 추정 절차
    1. 비용함수 정의
        
        `C(β₀, β₁) = Σ (yᵢ − (β₀ + β₁xᵢ))²`
        
    2. β₀, β₁에 대해 미분
    3. 미분값 = 0 → 최적해 도출
    
- 최종 공식
    - `β₁ = Σ (xᵢ − x̄)(yᵢ − ȳ) / Σ (xᵢ − x̄)²`
    - `β₀ = ȳ − β₁x̄`
        
        (β₁ = X와 Y가 함께 변하는 정도, X 변화에 따른 Y 변화량으로 해석 가능)
        
---

## 6. 잔차와 오차의 구분

### 예측값

- `ŷ = β̂₀ + β̂₁x`

### 잔차(Residual)

- `e = y − ŷ`
- 실제 값 − 예측값
- 데이터에서 직접 계산 가능
- 분석과 평가에서 사용하는 값

### 오차(Error)

- `ε = y − E(y)`
- 진짜 평균과의 차이
- 현실에서는 직접 관측 불가

⇒ 실제 분석에서는 잔차(residual)를 다룸

---

## 7. 선형회귀 실제 예제

### 데이터

- X: 주행속도
- Y: 제동거리

### 학습 결과

- `β̂₀ = −17.579`
- `β̂₁ = 3.932`
- `ŷ = −17.579 + 3.932x`

### 해석

- 주행속도가 1 증가할 때 제동거리는 약 3.932 증가

---

## 8. 모델 설명력 평가: 분산 분해와 R²

### 분산 분해

- `SST = SSR + SSE`
    - SST: 전체 변동
    - SSR: 모델이 설명한 변동
    - SSE: 설명하지 못한 변동
- 목적: 모델이 Y를 얼마나 설명하는지 평가

### 결정계수 R²

- `R² = SSR / SST = 1 − SSE / SST`
- 0~1 사이 값
- 1에 가까울수록 설명력이 큼

### R²의 한계

- 변수를 추가하면 R²는 거의 항상 증가
- 변수 개수를 고려한 **Adjusted R²** 사용
