## 다중공선성

### 1. 다중공선성의 정의

**설명변수(X)들끼리 너무 비슷하게 움직이는 현상** ex) 키 & 다리 길이

⇒ 모델 :  두 변수가 거의 똑같은 양상이니, 변수 별 중요도를 판별하지 못함

### 2. 다중공선성의 문제점

다중공선성이 심하면 **선형회귀 모델**에서 문제가 심화

- **계수(β) 불안정**
    
    데이터 조금만 바뀌어도 **가중치가 확 바뀜** ex) ****변수가 +였다가 다음엔 -로 바뀌는 식
    
- **변수 중요도 해석**
    
    실제로 중요한 변수인데도 **p-value가 커져서 안 중요해 보이거나**
    
    **표준오차가 커져서 확신이 없어지는** 상황 발생
    
- **예측 성능**
    
    중복된 정보를 너무 많이 넣으면 **과적합 유도**
    
    (특히 샘플 수가 적을수록 더 위험)
    

### 3. 다중공선성 판별법

1. **상관계수(Correlation)**
    
    1에 가까움 → 두 변수가 거의 똑같이 움직임
    
    **|corr|가 0.8~0.9 이상**이면 위험 신호
    
2. **VIF(분산팽창계수)**
    
    VIF가 클수록 이 변수는 다른 변수로 설명 가능
    
    - 기준 : 1~5: 괜찮음 / 5~10: 의심 / 10 이상: 심각
3. **회귀 결과**
    
    전체 모델 R² 점수는 높지만, 개별 변수 p-value는 낮은 경우
    

### 4. 해결 방법

- **변수 줄이기 :** 둘이 거의 같은 의미면 하나 빼기
- **Feature Selection :** 통계/모델 성능 기준으로 중요한 것만 남기기
- **Feature Extraction :** 비슷한 변수들을 ****PCA로 합쳐서 요약 변수로 바꾸기
- **정규화(Regularization) :**
    - Ridge(L2): 계수를 안정화시켜 다중공선성에 강함(다 남기지만 너무 튀는 변수를 눌러줌)
    - Lasso(L1): 중요 변수만 남기는 효과(나머지는 0으로 만들기)

---

## 차원의 저주

### 1. 차원의 저주의 정의

변수(차원)가 늘어날수록 데이터 공간이 엄청 넓어지고, 데이터는 상대적으로 듬성듬성해짐

⇒ 변수 많아질수록, 같은 수준의 학습이라도 데이터가 기하급수로 더 필요해짐

### 2. 차원의 저주의 문제점

- **데이터가 성글어짐(Sparsity)**
    
    2차원에서는 점들이 꽤 촘촘하지만, 100차원으로 가면 점들이 다 멀리멀리 흩어짐
    
    → 비슷한 애 찾기가 어려워짐
    
- **거리 개념이 망가짐**
    
    고차원에서는 가장 가까운 점과 가장 먼 점의 거리가 “비슷해지는” 현상이 생겨.
    
    → KNN 같은 거리 기반 알고리즘이 힘들어짐
    
- **과적합**
    
    변수가 많으면 모델이 외우기가 쉬워져서
    
    → 훈련 성능은 좋지만, 실제 성능은 좋지 않음
    
- **계산량 증가**
    
    변수 많으면 
    
    → 연산, 저장공간, 학습시간 증대
    

### 3. 해결 방법

- **변수 선택(Feature Selection)**: 필요 없는 변수를 버림
- **변수 추출(Feature Extraction)**: 변수를 합쳐서 새 변수로 요약

---

## 변수 선택

### 1. 변수 선택의 정의

원래 변수 X들 중에서 **필요한 변수만 골라서 남기는 것**

- 장점
    - 해석이 쉬움
    - 데이터 의미를 보존하기 좋음
    - 모델이 가벼워짐
- 단점
    - 버린 변수 조합이 사실 중요했을 수도 있음
    

### 2. 변수 선택 방법 3종류

- **Filter 방식**
    
    **모델 학습 전에** 통계적으로 걸러내기
    
    ex) 상관계수, chi-square, mutual information 등
    
    빠르지만, 모델 성능과 100% 일치하진 않을 수 있음
    
- **Wrapper 방식**
    
    변수 조합을 바꿔가며 **모델 성능으로 직접 평가**
    
    ex) Forward / Backward / Stepwise
    
    성능 기준이라 강력하지만, 계산 비용 큼
    
- **Embedded 방식**
    
    모델이 학습하면서 **자동으로 변수 선택**
    
    ex) Lasso, Tree 계열(Decision Tree, Random Forest) 등
    
    선택+학습이 동시에 이루어지지만, 특정 모델에 의존
    

---

## Forward / Backward / Stepwise

### 1) Forward Selection (전진 선택)

**아무 변수도 없는 상태**에서 시작해서 **하나씩 추가**

- Step1: 가장 좋은 변수 1개 넣기
- Step2: 그 상태에서 “추가로 넣었을 때 가장 좋아지는 변수” 넣기
- 반복…

**빠르지만, 한 번 넣으면 수정이 어려움**

### 2) Backward Elimination (후진 소거)

**모든 변수를 다 넣은 상태**에서 시작해서 **하나씩 제거**

- Step1: 가장 쓸모없는 변수 빼기
- Step2: 다음 쓸모없는 변수 빼기
- 반복…

**전체에서 시작하기 때문에 좋은 조합 찾을 확률이 높지만, 계산량이 크고 한 번 뺀 것은 수정 어려움**

### 3) Stepwise Selection (스텝와이즈)

**Forward + Backward 섞은 버전**

- 변수의 추가와 제거가 같이 이루어짐

**조합 품질이 좋아질 가능성 크지만, 시간이 오래 걸림**

### 4) 성격 비교

- 속도: **Forward > Backward > Stepwise**
- 결과 품질: **Stepwise > Backward > Forward**

---

## 변수 추출 (PCA,MDS)

### 1. 변수 추출의 정의

원래 변수들을 버리는 게 아니라, 섞어서 새 변수(Z)를 만들어서 요약

ex) Z1 = 0.4X1 + 0.3X2 + 0.1X3 …

- 장점
    - 정보 손실을 적게 하면서 차원을 크게 줄일 수 있음
    - 시각화(2D,3D) 엄청 강함
- 단점
    - Z1이 뭔 의미인지 해석이 어려움

### 2. PCA (Principal Component Analysis)

- 목표 : 데이터가 가장 많이 퍼져있는 방향(분산이 큰 방향)을 새 축으로 잡아서 요약
    - PC1: 가장 분산 큰 방향
    - PC2: PC1과 직교하면서 그다음 분산 큰 방향
    
    ⇒ 이렇게 축들을 만들고, 상위 몇 개만 남겨서 차원 축소
    
- 유리한 경우
    - 변수 많고 서로 상관관계 많을 때(중복 많을 때)
    - 시각화
    - 노이즈 줄이고 싶을 때

### 3) MDS (Multidimensional Scaling)

- 목표 : 데이터들 사이의 거리 관계(비슷/다름)를 최대한 유지하면서 2D/3D에 배치
    - 입력: 거리(또는 비유사성) 행렬
    - 출력: 2차원/3차원 좌표
- 유리한 경우
    - 샘플들의 유사도를 알고 싶을 때
    - 거리 기반 시각화/해석(포지셔닝 맵 느낌)

---

## 변수 선택 vs 변수 추출 차이

| 구분 | 변수 선택 (Feature Selection) | 변수 추출 (Feature Extraction) |
| --- | --- | --- |
| 한 줄 정의 | 기존 변수 중 **필요한 것만 고름** | 기존 변수를 **섞어서 새 변수 생성** |
| 목표 | 불필요한 변수 제거  | 정보 요약·압축  |
| 결과물 | 원래 변수 일부(X1, X7, X20 …) | 새로 만든 변수(Z1, Z2 …) |
| 변수 개수 | 줄어들긴 하지만 한계 있음 | **강하게 줄일 수 있음** |
| 해석 난이도 | 쉬움  | 어려움  |
| 정보 보존 | 제거된 정보는 사라짐 | **정보 최대한 유지** |
| 다중공선성 대응 | 일부 해결 가능 | **아주 잘 해결** |
| 대표 기법 | Filter / Wrapper / EmbeddedForward / Backward / StepwiseLasso | PCA, MDS |

---

### 실전

- 발표에서 “중요한 변수를 설명”해야 한다 → **변수 선택**
- 차원이 너무 커서 “요약/시각화/압축”이 목적이다 → **변수 추출(PCA/MDS)**
- 샘플 간 거리/유사성 관계가 중요하다 → **MDS**
- 상관이 큰 변수들이 많다 → **PCA 또는 Ridge/Lasso + 선택**
