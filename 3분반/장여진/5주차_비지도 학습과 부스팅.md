## 1. 군집화(Clustering) : k-means, DBSCAN

### 군집화의 정의

- **비지도 학습** : **정답이 없는 데이터**를 **비슷한 것끼리 묶는 것**
- **군집화** : 비지도 학습 중 하나 → 비슷한 특징을 가진 샘플들을 묶어서 k개 그룹으로 나눔
- 좋은 군집화의 기준
    - 같은 군집 안(intra)은 가까워야 함
    - 다른 군집 사이(inter)는 멀어야 함

### k-means

군집화의 기본 알고리즘, **중심점(평균)**으로 군집을 설정

- 과정
    1. 랜덤으로 중심점 k개 생성
    2. 각 점을 가장 가까운 중심에 배정
    3. 각 군집의 중심을 평균으로 업데이트
    4. 더 이상 배정이 안 바뀌면 종료
- 장점
    - 빠르고 단순
- 단점
    - **k를 미리 정해야 함**
    - **동그란 모양 군집**에만 잘 맞음
    - 이상치에 흔들릴 수 있음
    - 초기 중심에 따라 결과가 바뀔 수 있음

### DBCAN

모양, 이상치에 민감한 k-means의 단점을 보완, **밀도**로 군집을 설정

- 파라미터 2개
    - **eps :** 반경
    - **minPts**: 이웃 점 갯수
- 점 종류
    - **Core**: eps 안에 minPts 이상(군집 핵심)
    - **Border**: core 근처지만 본인은 core 기준 못 충족
    - **Noise**: 어디에도 안 붙는 점(버림)
- 장점
    - k 필요 없음
    - 이상치 자동 처리
    - 구불구불한 군집도 가능
- 단점
    - 밀도 다른 군집이 섞이면 파라미터 하나로 맞추기 어려움
    - eps 설정이 까다로움
    

---

## 2. Decision Tree: 결정트리, 불순도, gini-index, entropy

### 결정트리

**정답(y)이 있는 데이터**를 가지고, 

질문(조건)을 하나씩 던져가며 **데이터를 계속 나눠**서(YES/NO) 최종 예측을 만드는 모델

- 재귀적 분리
    - **처음엔 데이터가 한 덩어리(루트 노드)** 로 존재
    - 질문 하나로 **두 덩어리로 쪼개고**
    - 그 다음엔 전체를 다시 보는 게 아니라, **쪼개진 덩어리 각각에 대해 또 질문을 찾아서 또 쪼갠다**
    - 이걸 반복(= 재귀)해서 점점 더 세세하게 나눔

### 불순도

좋은 질문을 고르기 위해 고려하는 점수

한 노드에 클래스가 섞일수록 불순도가 크고 한쪽으로 몰릴수록 불순도가 낮음

→ 분기 후 불순도를 가장 크게 줄이는 질문을 고름

### gini-index / entropy

불순도를 숫자로 재는 공식

- gini-index
    - 계산이 빠르고 트리(CART)에서 많이 씀
    - 섞일수록 값이 커짐, 순수할수록 0에 가까움
- entropy
    - 정보의 혼란도
    - 섞일수록 엔트로피 커짐
    

---

## 3. Ensemble: Bagging, RF

### 앙상블

결정트리는 데이터가 조금만 바뀌어도 질문이 바뀌어서 전체 트리 구조가 완전히 달라질 위험 높음

→ 분산이 큼 (훈련 데이터 샘플이 조금만 달라져도 모델 결과가 크게 흔들림)

이를 해결하기 위해

조금씩 다른 결정트리들을 여러 개 만들고, 그 예측을 평균을 내어 통계적으로 분산을 줄이기 위한 전략

### Bagging

- 해결하려는 문제 : **결정트리의 높은 분산**
- 과정
    
    원래 데이터에서 **중복 허용 랜덤 샘플링(bootstrap)** 으로 서로 조금씩 다른 데이터셋 여러 개 생성
    
    각 데이터셋으로 **결정트리 하나씩 학습**
    
    분류는 다수결, 회귀는 평균으로 예측
    
- 결과
    - 어떤 트리는 실수해도
    - 다른 트리들이 보완
    - **전체 예측이 훨씬 안정됨**
- 문제
    
    특정 변수가 너무 강하면 대부분의 트리가 **비슷한 구조**가 됨 → 앙상블의 효과가 없음
    

### Random Forest

Bagging의 단점을 보완하기 위해 각 분기에서 **변수도 랜덤으로 일부만** 사용해서 트리들의 다양성을 증대

- Bagging: 데이터 랜덤
- RF: 데이터 랜덤 + 변수도 랜덤
- 효과
    - 트리들 간 **상관관계가 낮아짐**
    - 평균/투표의 효과가 커짐
    

---

## 4. Ensemble : Boosting, AdaBoost, GBM, XGBoost, LightGBM, CatBoost

### Boosting

모델들을 독립적으로 보아 평균을 내는 것을 넘어, 

**순차적으로 학습**하여 앞 모델이 틀린 부분을 다음 모델이 고치도록 하는 것

### AdaBoost, GBM

- AdaBoost
    - 틀린 데이터에 가중치를 더 줘서 다음 모델이 더 집중하게 함
    - 단점: 이상치/노이즈에도 집착할 수 있음
- GBM(Gradient Boosting Machine)
    - 틀린 정도(오차/잔차)를 다음 모델이 맞추도록 계속 더함
    - 성능 강하지만 튜닝 필요(학습률, 트리 수, 깊이 등)
    

### XGBoost, LightGBM, CatBoost

### GBM의 한계

- 트리를 많이 붙이거나, 데이터가 커지면 학습이 느려짐
- 트리 수, 깊이, 학습률 같은 걸 잘못 잡으면 과적합 위험이 큼
- 결측치, 범주형 변수, 불균형 데이터 등을 안정적으로 처리하도록 보완해야함

### 보완
| 항목 | **XGBoost** | **LightGBM** | **CatBoost** |
| --- | --- | --- | --- |
| 보완 | GBM을 **더 안정적으로 사용** | GBM을 **대용량에서도 빠르게** 학습 | GBM을 **범주형 변수 데이터 대비** |
| 핵심 강점 | 성능-안정성 밸런스 좋고 범용성이 큼 | 큰 데이터에서 학습 속도 매우 빠른 편 | 범주형 처리에 강해 전처리 부담↓ |
| 유용 | **표(tabular) 데이터 전반** | **행이 많거나(대용량)** 
피처가 많은 경우 | **카테고리** 변수가 많은 표 데이터 |
| 속도 성격 | 빠른 편(상황 따라 다름) | 보통 **가장 빠른 축**으로 많이 알려짐(대용량에서 강점) | 데이터/설정에 따라 다르지만, 
범주형 처리 포함하면 실무 체감이 좋을 때 많음 |
| 주의점 | 파라미터 많아서 과하게 튜닝하면 과적합 가능 | 작은 데이터에서 세팅 공격적이면 과적합/불안정 가능 | 범주형이 거의 없으면 장점이 덜 두드러질 수 있음 |
