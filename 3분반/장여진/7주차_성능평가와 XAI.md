# 결정계수, 수정결정계수

## 결정계수 ($R^2$)

1) 정의

**회귀모델이 실제 Y의 변동(흔들림)을 얼마나 설명했는지**를 0~1 사이 숫자로 나타낸 것

값이 **1에 가까울수록** 모델이 데이터를 잘 설명

$R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}$

2) 제곱합의 분해

회귀분석에서는 종속변수의 총 변동을 다음과 같이 분해

**SST = SSR + SSE**

- **SST** (Total) : 전체 변동
    
    $SST=\sum (Y_i-\bar{Y})^2$
    
- **SSR** (Regression) : 모델이 설명한 변동
    
    $SSR=\sum(\hat{Y}_i-\bar{Y})^2$
    
- **SSE** (Error) : 모델이 설명 못한 오차
    
    $SSE=\sum(Y_i-\hat{Y}_i)^2$
    

3) 특징

설명변수의 수가 증가하면, 해당 변수가 통계적으로 유의하지 않더라도 결정계수는 일반적으로 감소하지 않음 

→ 서로 다른 변수 개수를 갖는 모형을 단순히 결정계수로 비교하는 것은 적절하지 않음

## 수정결정계수 ($Adj,R^2$)

1) 도입 목적

불필요한 변수를 추가함으로써 인위적으로 결정계수가 증가하는 문제를 완화하기 위해 사용

2) 식

$Adj,R^2 = 1 - (1-R^2)\frac{n-1}{n-p-1}$

n: 샘플 수, p: 설명변수(피처) 개수

3) 특징

새로운 변수가 모형의 설명력을 충분히 향상시키지 못하면 수정결정계수는 감소

서로 다른 변수 개수를 가진 회귀모형을 비교할 때 결정계수보다 더 적절한 기준이 됨

→ 변수 선택 과정에서 모형 적합도의 비교 지표로 활용

---

# p-value, 분산 분석(ANOVA)

## p-value

1) 정의

데이터가 귀무가설과 얼마나 양립 가능한지를 수치로 나타낸 지표

유의수준 α=0.05를 기준으로

- p<α → 귀무가설 기각
- p≥α → 귀무가설 기각하지 않음

2) 회귀분석에서의 p-value

p-value가 충분히 작으면 해당 변수는 통계적으로 유의한 설명변수로 판단

## ANOVA(분산 분석) — 회귀 관점

1) 정의

회귀의 제곱합 분해를 표 형태로 정리하고, 회귀모형이 통계적으로 유의한지 검정

2) F-통계량

모든 설명변수가 종속변수에 영향을 주지 않는다는 가설→ 이를 검정하기 위해 F-통계량을 계산 

$F=MSE/MSR​$

- MSR=SSR/p (회귀 평균제곱)
- MSE=SSE/(n−p−1) (오차 평균제곱)

F값이 충분히 크면 설명된 변동이 오차 변동에 비해 상대적으로 크다는 의미

→ 귀무가설을 기각

---

# Confusion Matrix, F1 Score

## Confusion Matrix (혼돈행렬)

1) 정의

분류기의 예측 결과를  2x2(이진분류 기준)로 정리한 표

|  | 예측 Positive | 예측 Negative |
| --- | --- | --- |
| 실제 Positive | TP | FN |
| 실제 Negative | FP | TN |
- **TP (True Positive)**: 실제 양성(Positive)을 양성으로 예측
- **TN (True Negative)**: 실제 음성(Negative)을 음성으로 예측
- **FP (False Positive)**: 실제 음성을 양성으로 예측 (*Type I error, false alarm*)
- **FN (False Negative)**: 실제 양성을 음성으로 예측 (*Type II error, miss*)

2) 용도

혼동행렬은 단일 지표로는 파악하기 어려운 오류 유형(FP vs FN)의 trade-off를 명확히 보여줌

→ 불균형 데이터에서 필수적인 진단 도구

## F1 score

1) 배경

정확도(accuracy)는 클래스 불균형이 심한 상황에서 성능을 과대평가할 수 있음

양성 클래스 탐지 성능을 반영하는 지표가 필요하고, 그 대표가 Precision/Recall 및 F1

ex) 음성이 대부분인 데이터에서 항상 음성만 예측한다면 정확도가 높게 나옴

2)Precision, Recall 정의

- **Precision(정밀도)**: 양성으로 예측한 샘플 중 실제 양성의 비율
    
    $P=\frac{TP}{TP+FP}$
    
- **Recall(재현율/민감도/TPR)**: 실제 양성 샘플 중 양성으로 맞춘 비율
    
    $R=\frac{TP}{TP+FN}$
    

3) F1 정의(조화평균)

F1 score는 precision과 recall의 **조화평균**

$F1=\frac{2PR}{P+R}$

precision과 recall 값 중 하나가 매우 낮게 되면 F1이 크게 낮아짐

⇒ **precision–recall 균형을 요구하는 상황**에서 유용

4) 특징

- FP 비용과 FN 비용이 비슷하거나, 양성 탐지가 중요한 문제에서 자주 사용
- 비용이 비대칭이면 F1 단독보다 **Recall 우선 + 임계값 조정** 같은 접근이 더 적절할 수 있음

---

# PDP, ICE, LIME, SHAP (XAI)

아래 방법들은 일반적으로 사후 설명(post-hoc explanation)이며,

다수가 **모델 불문(model-agnostic)** 또는 **모델별 최적화 버전**을 가짐

## PDP (Partial Dependence Plot)

1) 정의

특정 특성(feature)가 예측값에 미치는 영향을 **다른 특성들을 주변화하여 평균 효과로 추정**한 함수

$PD(xs)=EXc[f(xs,Xc)]$

- $X_c$: 나머지 특성들
- f(⋅): 학습된 모델의 예측 함수

2) 특징

- **전역적(global) 평균 효과**를 설명 → 해당 feature가 변화할 때 평균적으로 예측의 변화를 설명
- 장점: 직관적, 평균 예측 변화가 한눈에 보임
- 단점: **특성 독립성(또는 약한 상관)** 가정에 취약, 개별 샘플의 이질적 반응이 숨겨질 수 있음
    
    

## ICE (Individual Conditional Expectation)

1) 정의

PDP의 평균화 이전 형태

각 개별 샘플 i에 대해 관심 특성  $x_s$를 변화시켰을 때의 예측 변화를 곡선으로 나타냄

$ICEi(xs)=f(xs,xc,i)$

2) 특징

- ICE는 **국소적(local-to-instance) 반응**을 다수 샘플에 대해 관찰하는 방식
- 곡선들이 서로 비슷 → 평균(PDP)이 대표성이 있음
- 곡선들이 크게 갈라지면 → **상호작용 또는 이질성**이 강하다는 신호

3) 단점

- 샘플 수가 많으면 시각적으로 난잡해질 수 있어, subsampling이나 대표 샘플 선택이 필요

## LIME

1) 정의

특정 예측 을 설명하기 위해, $x_0$ 주변에서 데이터를 교란하여 샘플을 생성하고,

그 주변에서 원 모델을 근사하는 해석 가능한 대체모델 g를 학습

$argg∈GminL(f,g,πx0)+Ω(g)$

- $\pi_{x_0}: x_0$ 근처 샘플에 더 큰 가중치를 주는 커널
- $Ω(g)$: 단순성(희소성 등) 제약
- $G$: 해석 가능한 모델 집합(주로 sparse linear model)

2) 특징

- LIME의 설명은 원 모델이 아니라 근사모델 g의 계수로 제공 → 해당 점 근처에서의 국소 근사

3) 한계

- 교란 방식(샘플 생성), 커널 폭, 특성 표현 방식에 따라 결과가 달라질 수 있어 설명 안정성 문제

## SHAP

1) 정의

협력 게임 이론의 Shapley value를 기반으로,

예측값을 기준값(base value) + 각 특성의 기여도로 분해

$f(x)=\phi_0+\sum_{j=1}^{p}\phi_j$

- $\phi_0$: 기준 예측(보통 전체 평균 예측)
- $ϕ_j:$ feature j의 기여도(Shapley value)

2) 해석

- $ϕ_j >0$: 해당 특성이 예측을 기준값보다 증가 방향으로 기여
- $ϕ_j <0$: 기준값보다 감소 방향으로 기여

3) 성질

- **Local accuracy (additivity)**: 기여도의 합이 예측을 재구성
- **Missingness**: 관측되지 않은 특성은 기여 0
- **Consistency**: 어떤 특성의 영향이 모델에서 증가하면 SHAP 기여도도 증가 방향으로 반영

4) 한계 

조합이 너무 많아서 원리 그대로 계산하면 비현실적으로 느림 

→ KernelSHAP, TreeSHAP 같은 빠른 버전이 등장
