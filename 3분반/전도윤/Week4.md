# 4주차

### 키워드 : 다중공선성, 차원의 저주, 전진선택, 후진소거, Stepwise, PCA, MDS

## 고차원 데이터
- 변수(피처) 수가 매우 많은 데이터
    Ex. 유전자 데이터, 4K 이미지, 추천 시스템 ... 등등

### 넷플릭스 추천 시스템이 왜 고차원 데이터일까 ? 
- 추천 시스템 데이터는 보통 행렬 ...
    행 : 사용자
    열 : 영화/드라마
    값 : 평점, 시청 여부, 클릭 수, 시청 시간 등 ...
    -> 사용자 1000만명 x 콘텐츠 10만 편이면 행렬 크기가 10,000,000 x 100,000 ...
    이때 한 사용자를 하나의 데이터 포인트로 보면 그 사용자는 콘텐츠 10만 개에 대한 정보를 변수로 가진 셈이라 특성(변수) 수 = 10만인 데이터 ...
        -> 사용자 벡터의 차원이 엄청 커지기 때문에 고차원 데이터

## 차원의 저주
- 변수가 증가할수록(차원이 증가할수록) 같은 수준으로 데이터를 설명하기 위해 필요한 데이터 개수가 기하급수적으로 증가 
    즉, 차원이 커지면 데이터가 부족해지고, 모델 학습이 급격히 어려워짐

1. 차원의 저주가 만드는 4가지 문제
- 데이터 밀도 감소
    차원이 커지면 공간의 부피가 폭발적으로 커져서 같은 개수의 데이터로는 공간을 촘촘히 채울 수 없게 됨(희소해짐). 그 결과 계산량이 늘고 모델은 제한된 샘플에 과하게 맞추려 하면서 과적합 위험이 커짐
- 거리 정보 왜곡
    차원이 커질수록 유클리드 거리 같은 '최단 거리'가 관측치 간 거리를 모두 비슷비슷하게 왜곡시킴
        WHY? 
        차원이 늘어나면 거리 계산은 각 차원의 차이를 제곱해서 더한 값(유클리드 거리)이 됨.
        여기서 차원이 커질수록 각 차원에서 생기는 작은 차이들이 전부 합쳐지면서 대부분의 점쌍이 비슷한 크기의 합을 갖게 됨. 따라서 거리의 분포가 한 값 근처로 몰리기에 가까움/멀음이 구별이 되지 않음
    그래서 고차원에서 군집화를 하면 대부분 단일 군집이 제일 좋은 것처럼 나오는 현상이 생길 수 있음
- 거리 정보 왜곡에 따른 예측 저하
    관측치가 구 내부에 존재할수록(중심점과의 거리가 가까울수록) 서로 간의 거리 비교에 의미가 있음
    즉, 코너에 있을수록 너무 큰 값이라 data space 상 분류에 어려움
        차원이 증가할수록 구의 상대적 비율이 줄어 관측치가 코너 근처에 있을 확률이 커지고 예측이 더 어려워짐
- 차원 중복
    변수 간 의미 중복에 따른 불필요한 연산량 증폭, 가정사항 위반 및 학습 모델의 정확도 저하
## 차원 축소 : 왜/무엇을 하는가
- 변수의 개수를 줄이는 기법
    핵심 효과
    - 차원의 저주 회피
    - 중복/불피요 변수 제거로 더 나은 모델 구축 가능
    - 데이터의 내재 특징을 보기 위한 EDA
### 변수 선택 VS 변수 추출
- 변수 선택
    원래 변수들 중에서 중요한 것만 골라 남김
    결과는 원래 피처의 부분 집합
- 변수 추출
    원래 변수를 변환/조합해서 새로운 축을 만들어 차원을 줄임
    Ex. PCA처럼 새로운 주성분을 만드는 방식
## 변수 선택 기법의 분류 : Filter, Wrapper, Embedded
### Filter
- 모델을 학습시키기 전에 통계적 기준으로 변수를 고름
    장점 : 빠름
    단점 : 모델 성능과의 직접 연결이 약할 수 있음
### Wrapper
- 변수 조합을 바꿔가며 모델 성능을 직접 평가하면서 고름
    장점 : 성능 기준으로 선택하니 강력
    단점 : 계산량 큼
### Embedded
- 모델 학습 과정 안에 변수 선택이 포함됨
    장점 : 비교적 효율적, 성능 좋음
    단점 : 모델 의존적
## 전진선택/후진소거/Stepwise
전역 탐색은 차원이 커지면 불가능하기에 휴리스틱(근사) 접근이 필요함 ...
### 전진 선택
- 빈 모델에서 시작해서 변수를 하나씩 추가
    한 번 넣은 변수는 다시 제거 불가(단조 증가)
    종료 조건
    - F검정에 따라 모델의 p-value 값이 가장 작은 값을 찾을 때까지 탐색
        - p-value란 추가하려는 변수 x가 정말로 y를 설명하는 데 도움이 되는지(우연이 아닌지)를 통계적으로 검사한 값
            p-value가 작다 : 이 변수가 y를 설명하는 효과가 있다고 보기 좋다
            p-value가 크다 : 추가해도 우연일 가능성이 크다
        - F검정은 변수를 추가하기 전 모델 vs 변수를 추가한 후 모델을 비교
            변수 하나 더 넣었더니 오차가 의미 있게 줄었는가 ? 
            의미 있게 줄었다면 그 변수는 유용
    - 예측 모델의 validation 예측 결과가 지속 좋아질 때까지 탐색
### 후진 소거
- 모든 변수를 포함한 모델에서 시작 -> 의미 없는 변수를 하나씩 제거
    한 번 뺀 변수는 다시 추가 불가
    종료 조건도 전진 선택과 유사하게 p-value/validation 개선 기준을 사용
### Stepwise
- 전진했다가 필요하면 후진도 하면서 왔다 갔다 반복
    변수 수가 단조 증가/감소하지 않고, 더 넓은 공간을 탐색 가능
    종료 조건 또한 p-value/validation 개선 기준
#### 세 방법 비교
- 수렴 속도 : 전진 > 후진 > Stepwise
- 좋은 변수 조합 : Stepwise > 후진 > 전진
    전진이 후진보다 빠르지만 후진이 더 좋은 해가 나올 수 있음 ...
## 다중공선성
- 회귀 분석에서 독립 변수들끼리 서로 너무 밀접하게 연결되어 있을 때(상관 관계가 높을 때) 발생하는 문제
    원래 독립 변수들은 서로 독립적으로 결과에 영향을 주어야 하는데, 변수들이 너무 인접해서 누가 결과에 진짜 영향을 주는지 구분하기 힘들어지는 상태
- 따라서, 중복 변수 제거/Ridge/LASSO/PCA 같은 방법으로 해결
## PCA
- 분산이 가장 큰 방향부터 새 축을 만들고, 상위 몇 개 축만 남겨 차원을 줄임
    결과 : 원래 피처들의 선형 결합으로 만든 새 피처(주성분)
    왜 표준화가 필요할까 ?
        스케일 큰 변수가 분산을 지배해서 PCA 축이 왜곡됨
    주성분은 서로 어떤 관계 ?
        서로 직교하도록 만들어 중복을 줄임
    몇 개 성분을 남기냐 ?
        누적 설명분산비율 기준으로 선택
## MDS(다차원 척도법)
- 데이터 간 거리(또는 비유사도) 행렬이 주어졌을 때, 그 거리를 최대한 보존하도록 2D/3D 좌표를 찾음(주로 시각화/구조 파악)
    PCA와의 차이 : PCA는 원본 피처 값에서 출발, MDS는 거리에서 출발
### 고차원에서 거리 왜곡 문제가 있으니 정의/스케일링이 매우 중요함 !!

## 문제 1
- 차원의 저주가 만드는 4가지 문제를 서술하시오

## 문제 2
- 차원 축소를 해주는 이유를 설명하시오
