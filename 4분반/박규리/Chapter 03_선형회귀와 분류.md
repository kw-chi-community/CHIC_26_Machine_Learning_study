# 선형회귀
## 수치 예측
- 반응변수가 연속형 숫자일 때 사용
## 범주 예측
- 반응변수가 범주형일 때 사용

## 변수 사이의 관계
- 확정적 관계 : Y = f(x)
- 확률적 관계 : Y = f(x) + ε(오차항)

## 단순 선형회귀
- Y = β0 + β1X (독립변수 1개)
## 다중 선형회귀
- 독립변수가 2개 이상

## 파라미터 추정
- 비용 함수 : 예측값과 실제값의 차이를 최소화
- 볼록 함수 : 아래로 볼록한 모양
- 최소제곱법 : B에 대해 미분하여 0이 되는 값을 찾는 방

# Loss Function(비용함수)
- 정의: 학습 데이터에서 모델의 예측값과 실제값의 차이를 수치화한 함수
- SSE : 선형회귀에서는 오차 제곱의 합을 비용 함수로 사용
- 결정계수(R^2) : 전체 변동 중 모델이 오차를 얼마나 줄였는지를 나타내는 지표

# Linear Regression(LR)
- 대상 : Y가 수치형일 때 사용하는 분류 모델
- 목적 : 입력과 출력의 관계를 선형식으로 설명하고 값을 예측
- 오차항의 정규성, 등분산성, 독립성, 선형성을 만족

# Logit(로지스틱 회귀) & SVM(서포트 벡터 머신)
- 대상 : Y가 범주형일 때 사용하는 분류 모델
- 선형회귀와 달리 Y를 범주로 예측한다는 점에서 차이가 있음
---
# 선형회귀모델
- 출력변수 Y를 입력변수 X들의 선형결합으로 표현한 모델
## 추정량의 성질
- 점추정
- 구간추정
## 가우스-마르코프 정리
- BLUE 성질
- 불편성
- 최량성
# 가설 검정
## 가설 설정
- 귀무가설 : 기울기 = 0, X는 Y에게 영향 X
- 대립가설 : 기울기 != 0, X는 Y에게 영향 O
## 판단 방법
- t값 : 기울기 추정값을 표준오차로 나눈 값
- p값 : H0가 참일 경우, 0.05보다 작으면 귀무가설을 기각

# 분산 분석
- SST
- SSR
- SSE
- 관계식 : SST = SSR + SSE
---
# Ridge 회귀
- 비용 함수에 계수의 제곱 합을 더함
- 다중공선성 문제 해결에 탁월
# Lasso 회귀
- 비용 함수에 계수의 절댓값 합을 더함
- 중요하지 않은 변수의 계수를 완전히 0으로 만듬
# Elastic Net
- Ridge와 Lasso의 페널티를 동시에 적용

# Bias-Variance Trade-off
- 가정이 성립할 때, 최소제곱법으로 구한 추정량은 편향이 없고 분산이 가장 작은 최적의 추정량
- 다중공선성 : 변수가 상관관계가 높으면, 분산이 지나치게 커짐
## Trade-off의 적용
- Ridge나 Lasso 같은 규제 모델 적용 -> 약간의 편향을 추가, 분산을 줄여 오차를 낮춤
