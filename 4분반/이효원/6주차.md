# CHIC 기계학습 스터디 6주차 정리본

기계학습 스터디 4분반 이효원

## 활성화 함수 (Activation Function): Sigmoid와 ReLU

인공 신경망에서 활성화 함수(activation function)는 가중합 결과를 비선형으로 변환해, 모델이 단순 선형 분리 이상을 학습할 수 있게 하는 핵심 구성요소다.  
활성화 함수가 없으면 여러 층을 쌓더라도 결국 하나의 선형 모델과 크게 다르지 않다.

### Sigmoid

Sigmoid 함수는 입력값을 0과 1 사이로 압축하는 대표적인 활성화 함수다.

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

#### 특징

- 출력 범위가 [0, 1]이라 확률처럼 해석하기 쉽다.
- 초기 신경망에서 널리 사용되었고, 이진 분류 출력층에서 지금도 자주 사용된다.
- 입력 절댓값이 커지면 기울기가 매우 작아져 학습 속도가 느려질 수 있다. (vanishing gradient)

### ReLU

ReLU(Rectified Linear Unit)는 음수 입력은 0으로, 양수 입력은 그대로 통과시키는 함수다.

$$
\mathrm{ReLU}(z) = \max(0, z)
$$

#### 특징

- 계산이 단순하고 학습 속도가 빠르다.
- 양수 구간에서 기울기가 유지되어 깊은 네트워크 학습에 유리하다.
- 음수 구간에서는 출력이 0이라 일부 뉴런이 비활성 상태로 고정되는 문제가 생길 수 있다. (dying ReLU)

> Sigmoid보다 ReLU가 학습 속도 측면에서 빠른 활성화 함수이다.

## SNN, DNN, AE

### SNN (Shallow Neural Network)

SNN(Shallow Neural Network)은 일반적으로 은닉층이 1개(또는 매우 얕은 구조)인 신경망을 뜻한다.  
구조가 단순해 해석이 상대적으로 쉬우며, 데이터 규모가 크지 않을 때는 충분한 성능을 보이기도 한다.

#### 한계

- 표현력(복잡한 패턴 학습 능력)이 제한적이다.
- 고차원/복잡한 문제에서는 성능 한계가 빠르게 드러난다.

### DNN (Deep Neural Network)

DNN은 은닉층을 여러 개 쌓은 신경망으로, 계층적으로 특징을 추출한다.  
즉, 낮은 수준 특징에서 높은 수준 특징으로 점진적 추상화가 가능하다.

#### 장점

- 복잡한 비선형 관계를 학습하는 능력이 높다.
- 이미지, 음성, 텍스트처럼 고차원 데이터에서 강력한 성능을 낸다.

#### 대표 이슈

- Vanishing gradient: 층이 깊어질수록 기울기 소실이 발생하기 쉽다.
- Overfitting: 파라미터 수가 많아 과적합 위험이 증가한다.

#### 완화 전략

- 정규화(Regularization), Dropout, Early stopping
- Batch Normalization
- 가중치 초기화(Weight Initialization) 개선
- 적절한 최적화 기법(Adagrad, RMSProp 등)

### AE (AutoEncoder)

AutoEncoder는 입력을 출력으로 최대한 복원하도록 학습하는 비지도 신경망이다.  
핵심은 단순 복사가 아니라, 중간 잠재공간(latent code)에 입력의 중요한 표현을 압축해 담는 것이다.

#### 기본 구조

- Encoder: 입력을 잠재표현으로 압축
- Latent Code: 압축된 핵심 표현
- Decoder: 잠재표현으로부터 원래 입력을 복원

#### 활용

- 차원 축소
- 특징(표현) 학습
- 생성 모델링(Decoder 기반)

## RNN, LSTM, Seq2Seq

순서 정보가 중요한 데이터(문장, 시계열 등)는 각 시점이 독립이 아니므로, 이전 정보의 맥락(context)을 반영하는 모델이 필요하다.

### RNN (Recurrent Neural Network)

RNN은 이전 시점의 은닉상태를 현재 계산에 재사용하는 순환 구조를 가진다.  
즉, 현재 출력이 과거 입력 이력에 의존할 수 있게 설계된 신경망이다.

#### 핵심 아이디어

- 시점 $t$에서 은닉상태 $h_t$는 현재 입력 $x_t$와 이전 은닉상태 $h_{t-1}$로 계산된다.
- 이 구조 덕분에 시퀀스(문장/시계열) 데이터에 자연스럽게 대응한다.

#### 한계

- BPTT(Backpropagation Through Time) 과정에서 장기 의존성 학습이 어렵다.
- 특히 기울기 소실(vanishing gradient)로 먼 과거 정보를 유지하기 어렵다.

### LSTM (Long Short-Term Memory)

LSTM은 RNN의 장기 의존성 문제를 완화하기 위해 게이트 구조를 도입한 모델이다.  
핵심은 cell state를 통해 정보를 선택적으로 유지/삭제/갱신하는 데 있다.

#### 게이트 기반 제어

- Forget gate: 과거 정보 중 무엇을 버릴지 결정
- Input gate: 현재 입력 정보를 얼마나 반영할지 결정
- Output gate: 현재 시점 출력으로 무엇을 내보낼지 결정

#### 장점

- 단순 RNN 대비 long-term dependency 학습 성능이 좋다.
- 자연어 처리, 시계열 예측 등 순차 데이터에서 안정적인 성능을 보인다.

### Seq2Seq (Sequence-to-Sequence)

Seq2Seq는 입력 시퀀스를 다른 출력 시퀀스로 변환하는 인코더-디코더 구조다.  
대표적으로 기계번역에서 많이 사용된다.

#### 기본 구조

1. Encoder가 입력 시퀀스를 읽어 문맥 벡터(Context Vector)로 요약한다.
2. Decoder가 이 문맥 벡터를 이용해 출력 시퀀스를 순차적으로 생성한다.

#### 특징과 한계

- 입력 길이와 출력 길이가 달라도 처리 가능하다.
- 고정 길이 Context Vector만 사용할 때는 긴 문장에서 정보 병목이 발생할 수 있다.
- 이 한계를 완화하기 위해 Attention 메커니즘이 함께 사용된다.
