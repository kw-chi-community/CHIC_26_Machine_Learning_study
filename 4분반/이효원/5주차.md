# CHIC 기계학습 스터디 5주차 정리본

기계학습 스터디 4분반 이효원

## 군집화 알고리즘: K-평균 및 DBSCAN

군집화(clustering)는 정답 라벨이 없는 데이터에서,  
데이터를 유사한 특성을 가진 그룹으로 묶는 비지도 학습 기법이다.  
그중에서도 K-평균(K-Means)과 DBSCAN은 대표적인 방법이므로 함께 정리한다.

### K-평균 군집화 (K-Means Clustering)

K-평균은 데이터를 미리 정한 $K$개의 군집으로 나누는 분리형 군집화 알고리즘이다.  
각 군집은 하나의 중심(centroid)을 가지며, 각 데이터는 가장 가까운 중심에 할당된다.  
따라서 학습 전에 군집 개수 $K$를 지정해야 한다.

#### 기본 원리

K-평균은 아래 과정을 반복하면서 군집 중심을 갱신한다.

1. 초기 중심 설정: 데이터 공간에서 $K$개의 점을 임의로 선택해 초기 중심으로 설정한다. (초기값은 최종 결과에 영향을 줄 수 있다.)
2. 할당: 각 데이터에 대해 가장 가까운 중심(보통 유클리드 거리)을 찾아 해당 군집에 할당한다.
3. 중심 업데이트: 각 군집에 들어간 데이터들의 평균 위치로 중심을 다시 계산한다.
4. 종료: 중심이 더 이상 변하지 않거나, 할당이 더 이상 바뀌지 않으면 종료한다. (또는 최대 반복 횟수 도달 시 종료)

#### 군집 수 K 결정 및 평가 지표

K-평균의 결과는 $K$에 따라 달라질 수 있으므로, 적절한 $K$를 선택하는 것이 중요하다.

##### 군집 수 결정:

군집 수를 결정하는 방법 중에는 대표적으로 엘보우(Elbow) 방법이 있다. 이는 여러 $K$에 대해 SSE를 계산해 그래프로 그린 뒤, $K$ 증가에 따른 SSE 감소 폭이 급격히 줄어드는 지점을 선택하는 방법이다.

##### 평가 지표:

- 오차 제곱합(Sum of Squared Errors, SSE): 각 군집 내 관측치와 해당 군집 중심 간 거리의 제곱을 합한 값이다.

  $$
  SSE = \sum_{i=1}^{K}\ \sum_{x \in C_i} dist(x, c_i)^2
  $$

  SSE가 작을수록 군집 내 관측치가 중심에 더 가깝게 밀집되어 있음을 의미한다.

- 실루엣(Silhouette) 계수: 각 관측치가 자신이 속한 군집에 얼마나 잘 포함되어 있는지, 그리고 다른 군집과는 얼마나 잘 분리되어 있는지를 나타내는 지표다.  
   각 관측치 $i$에 대해

  $$
  s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}
  $$

  로 계산한다.
  - $a(i)$: 관측치 $i$가 속한 군집 내 다른 관측치들과의 평균 거리 (작을수록 응집력이 높음)
  - $b(i)$: 관측치 $i$와 가장 가까운 다른 군집의 관측치들과의 평균 거리 (클수록 분리도가 높음)

  실루엣 계수 값은 -1에서 1 사이의 범위를 가지며, 0.5보다 크면 군집 결과가 비교적 옳다고 판단할 수 있다. 1에 가까울수록 군집화가 잘 되었음을 의미하고, -1에 가까울수록 관측치가 잘못된 군집에 할당되었을 가능성이 크다.  
  가능성이 큰 것이지 잘못된 군집에 할당되었다고 단언할 수는 없다.(비지도 학습 데이터의 특성)

### DBSCAN (Density Based Spatial Clustering of Applications with Noise)

DBSCAN은 밀도(density) 기반 군집화 알고리즘이다.  
K-평균처럼 군집 개수 $K$를 미리 정하지 않아도 되며, 노이즈(Noise)를 식별하는 데 강점이 있다.  
또한 K-평균이 거리 기반으로 구형(spherical) 군집을 선호하는 경향이 있는 반면, DBSCAN은 다양한 형태의 군집을 찾는 데 유리하다.

#### 핵심 원리 및 하이퍼파라미터

##### 하이퍼파라미터

- $\varepsilon$ (Epsilon): 이웃 관측치를 탐색하는 최대 반경
- `MinPts` (Minimum Points): $\varepsilon$ 반경 내에서 Core 포인트로 분류되기 위한 최소 이웃 관측치 수

##### 관측치 분류

이 두 하이퍼파라미터를 바탕으로 DBSCAN은 데이터 내의 각 관측치를 다음 세 가지 유형 중 하나로 분류한다.

- Core(코어) 포인트: $\varepsilon$ 반경 내에 `MinPts` 이상의 이웃 관측치를 포함하는 점
- Border(보더) 포인트: $\varepsilon$ 반경 내 이웃 관측치 수는 `MinPts` 미만이지만, 어떤 Core 포인트의 $\varepsilon$-이웃에 포함되는 점
- Noise (노이즈) 포인트: Core 포인트도 Border 포인트도 아닌 점이다. 즉, 주변에 충분한 이웃이 없거나 Core 포인트와 연결되지 않은 점으로, 군집에 속하지 않는 이상치로 간주된다.

#### 군집 형성 과정

1. 데이터셋에서 임의의 점 $P$를 선택한다.
2. $P$가 Core 포인트라면, $P$로부터 밀도-도달 가능(density-reachable)한 점들을 포함하도록 군집을 확장한다. (Core를 통해 연결되는 Core/Border 포인트가 하나의 군집을 이룬다.)
3. $P$가 Border 포인트라면, 군집 확장을 수행하지 않고 다음 점으로 진행한다.
4. $P$가 Noise 포인트라면, 노이즈로 분류하고 다음 점으로 진행한다.
5. 데이터셋의 모든 점이 처리될 때까지 이 과정을 반복한다.

#### 장단점

##### 장점

- 노이즈에 강함: 노이즈 포인트를 명시적으로 식별하여 군집에서 제외할 수 있다.
- 임의 모양 군집: 구형이 아닌, 다양한 기하학적 모양의 군집도 효과적으로 찾아낼 수 있다.
- 군집 수 불필요: K-평균과 달리 군집의 수를 미리 지정할 필요가 없다.

##### 단점

- 밀도 다양성: 데이터셋 내에 밀도가 크게 다른 군집들이 존재하는 경우, 단일 ε와 MinPts 값으로 모든 군집을 잘 찾아내기 어렵다.
- 하이퍼파라미터 민감도: ε와 MinPts 값 설정에 따라 군집 결과가 크게 달라질 수 있어, 최적의 하이퍼파라미터를 찾는 것이 어려울 수 있다.
- 차원의 저주: 고차원 데이터에서는 밀도 개념 자체가 모호해지기 때문에, 성능이 저하될 수 있다.

## 의사결정나무 및 불순도

의사결정나무(Decision Tree)는 분류/회귀 모두에 쓰이는 직관적인 지도 학습 알고리즘이다.  
특정 변수(특성)에 대한 조건을 순차적으로 적용해 데이터를 분할하고, 그 결과를 나무 형태의 규칙으로 표현한다.

### 의사결정나무

#### 기본 구조

의사결정나무는 Tree 구조로 표현되며, 구조 자체가 비교적 해석 가능하다는 장점이 있다.

- Root node (뿌리마디): 나무의 가장 상단에 위치하며, 데이터셋 전체를 대표하는 시작점이다.
- Internal node (중간마디): 특정 변수(특성)에 대한 조건을 바탕으로 데이터를 분할하는 노드이다. 각 중간마디는 하나의 질문(예: '나이가 30세 이상인가?')을 나타낸다.
- Branch: 중간마디의 질문에 대한 답(예: '예', '아니오')에 따라 데이터가 나뉘어지는 경로를 의미한다.
- Leaf node (끝마디): 더 이상 분할되지 않고, 최종적으로 분류 결과(클래스 라벨) 또는 예측 값(회귀)을 결정하는 노드이다. 각 끝마디는 특정 규칙을 만족하는 데이터들이 모인 그룹을 나타낸다.

#### 작동 원리

학습은 데이터를 가장 잘 분리하는 분할 기준을 반복적으로 찾는 재귀 과정이다.

1. 시작: Root node에서 전체 데이터로 시작한다.
2. 분할 기준 탐색: 현재 노드를 가장 순수하게(균질하게) 나누는 변수/임계값을 찾는다.  
   이때 순수성을 정량화하는 지표가 불순도(Impurity)다.
3. 데이터 분할: 선택한 기준으로 데이터를 하위 그룹으로 나눈다.
4. 반복: 각 하위 그룹에서도 2단계와 3단계를 반복한다.
5. 종료: 아래 조건 중 하나라도 만족하면 더 이상 나누지 않고 Leaf node가 된다.
   - 더 이상 분할할 변수가 없을 때.
   - 모든 데이터가 한 클래스에 속하여 불순도가 0이 될 때.
   - 노드에 포함된 데이터 개수가 미리 정해진 최소값보다 작을 때.
   - 나무의 깊이가 미리 정해진 최대 깊이에 도달했을 때.

### 불순도 (Impurity)

불순도는 특정 노드에 포함된 데이터가 얼마나 혼합되어 있는지(동질적이지 않은지)를 나타내는 값이다.  
불순도가 높으면 여러 클래스가 뒤섞여 있다는 뜻이고, 의사결정나무는 분할 후 불순도가 최대한 낮아지도록 가지를 나눈다.

#### 불순도 측정 방법

의사결정나무 알고리즘은 노드의 불순도를 측정하고, 분할을 통해 이 불순도를 얼마나 감소시킬 수 있는지를 평가하여 최적의 분할 지점을 찾는다. 대표적인 불순도 측정 방법은 다음과 같다.

##### 지니 점수 (Gini Index)

- 데이터의 통계적 분산 정도를 정량화한 값으로, 특정 노드에서 임의로 추출된 두 개의 개체가 서로 다른 클래스에 속할 확률을 나타낸다.
- 공식: $IG = 1 - \sum_{j=1}^{m} p_j^2$
  - $m$은 클래스 개수, $p_j$는 현재 노드에서 클래스 $j$ 비율
- 지니 점수는 0에 가까울수록 순수도가 높고(즉, 불순도가 낮고), 0.5에 가까울수록 불순도가 높다(완전히 섞여 있을 때). 예를 들어 특정 노드의 모든 개체가 하나의 클래스에 속한다면 $p_j=1$이 되어 $IG = 1 - 1^2 = 0$이 된다.

##### 엔트로피 (Entropy)

- 정보 이론에서 유래한 개념으로, 데이터의 혼잡도 또는 불확실성을 측정하는 지표이다. 불순도가 높을수록 더 많은 정보량을 가진다고 해석할 수 있다.
- 공식: $IE = -\sum_{j=1}^{m} p_j \log_2(p_j)$
  - $m$은 클래스 개수, $p_j$는 현재 노드에서 클래스 $j$ 비율
- 엔트로피도 0에 가까울수록 순수도가 높고(불순도가 낮고), 값이 클수록 불확실성이 크다(불순도가 높다).  
  모든 개체가 한 클래스라면 $p_j=1$이므로 $IE = -1 \cdot \log_2(1) = 0$이 된다.

## 앙상블 학습 기법: Bagging 및 부스팅

앙상블(ensemble)은 여러 개의 학습 모델(약한 학습기, weak learner)을 결합해 예측 성능을 향상시키는 방법이다.  
단일 모델의 한계를 완화하고 일반화 성능을 개선하는 것이 목적이다.  
대표적인 큰 갈래는 Bagging / Boosting / Stacking이다.

### Bagging (Bootstrap Aggregating)

Bagging은 Bootstrap + Aggregating의 합성어이다.  
데이터 샘플을 다양하게 구성해 여러 모델을 병렬 학습한 뒤, 예측 결과를 집계하여 최종 예측을 계산한다.  
주로 분산(variance)을 줄여 과적합을 완화하는 데 효과적이다.

#### 작동 원리

1. 데이터 샘플링(Bootstrap): 학습 데이터에서 복원 추출(sampling with replacement)로 여러 개의 부트스트랩 샘플을 생성한다.  
   각 샘플은 크기는 같지만 구성은 달라질 수 있으며, 이는 모델 간 다양성 확보에 기여한다.
2. 약한 학습기 학습: 각 샘플로 독립적인 모델(예: 의사결정나무)을 학습한다. (병렬 학습 가능)
3. 결과 집계(Aggregating): 여러 모델의 예측을 모아 최종 예측을 만든다.
   - 분류(Classification) 문제의 경우: 대부분 '다수결 투표(Majority Voting)' 방식을 사용하여 가장 많은 모델이 예측한 클래스를 최종 결과로 선택한다.
   - 회귀(Regression) 문제의 경우: 각 모델의 예측 값들을 '평균(Averaging)'하여 최종 예측 값을 도출한다.
   - 수식으로는 $\hat{y} = \text{Aggregating}(\hat{f}_1(x), \hat{f}_2(x), \dots, \hat{f}_M(x))$ 로 표현할 수 있다.

#### 다양성 확보

Bagging은 각 약한 학습기에게 서로 다른(무작위) 훈련 데이터 하위 집합을 제공하여 모델 간 다양성을 만든다.  
이 다양성 덕분에 개별 모델이 서로 다른 부분에서 실수하더라도, 집계 과정에서 오류가 어느 정도 상쇄된다.  
대표적인 Bagging 알고리즘으로는 '랜덤 포레스트(Random Forest)'가 있다.

### Boosting

부스팅은 Bagging과 달리 약한 학습기를 순차적으로 학습시키는 앙상블 기법이다.  
각 단계의 모델은 이전 모델의 오차를 보정하는 방향으로 학습된다.  
주로 편향(bias)을 줄여 예측 성능을 향상시키는 데 초점이 있다.

#### 작동 원리

1. 초기 모델 학습: 첫 번째 약한 학습기 $f_1(x)$를 학습한다.
2. 오차 반영: 예측과 정답의 차이(잔차)를 바탕으로 관측치에 가중치를 부여하며, 오차가 큰 관측치에 더 큰 가중치를 부여한다.
3. 새로운 모델 학습: 업데이트된 가중치를 반영해 다음 약한 학습기 $f_2(x)$를 학습한다.
4. 반복: 이 과정을 $M$번 반복해 $M$개의 약한 학습기를 순차적으로 구축한다.
5. 최종 예측: 각 약한 학습기의 예측을 가중치 합산해 최종 모델 $\hat{F}(x)$를 구성한다.

#### 핵심 아이디어

부스팅은 이전 모델에서 오차가 컸던 관측치에 더 큰 가중치를 부여하여, 후속 모델이 해당 관측치를 상대적으로 더 많이 학습하도록 만든다. 이를 통해 전체 성능을 점진적으로 개선한다.

#### AdaBoost (Adaptive Boosting)

AdaBoost는 초기 부스팅 알고리즘 중 하나로,  
오분류된 관측치에 더 큰 가중치를 부여하는 방식으로 순차 학습을 진행한다.

##### 순차적 학습 방식

1. 초기 가중치 설정: 모든 관측치에 동일한 초기 가중치(보통 $1/N$)를 부여한다.
2. 약한 학습기 학습 + 신뢰도 계산:
   - 현재 가중치로 약한 학습기(예: Decision Stump, 깊이 1 의사결정나무)를 학습한다.
   - 분류 오류율(가중치 합) $\epsilon_i$를 계산한다.
   - 신뢰도(가중치) $\alpha_i$를 계산한다.
     $$
     \alpha_i = \frac{1}{2}\ln\left(\frac{1-\epsilon_i}{\epsilon_i}\right)
     $$
     성능이 좋을수록($\epsilon_i$가 작을수록) $\alpha_i$가 커진다.
3. 관측치 가중치 업데이트:
   - 정분류한 관측치의 가중치는 감소시키고, 오분류한 관측치의 가중치는 증가시킨다.
   - 예시로 $w^{new} = w^{old}\times e^{-\alpha_i \times C}$ 형태로 업데이트할 수 있다. (정분류/오분류 여부에 따라 $C$의 부호가 바뀜)
4. 반복 및 최종 예측: 위 과정을 반복하고, 마지막에는 각 약한 학습기의 예측에 $\alpha_i$를 곱해 합산한 값으로 최종 예측을 계산한다.

#### Gradient Boosting Machine (GBM)

GBM(Gradient Boosting Machine)은 부스팅을 경사 하강법(Gradient Descent) 관점에서 해석한 알고리즘이다.  
각 단계에서 새 모델이 이전 모델의 잔차(residual), 혹은 손실 함수의 음의 기울기(negative gradient)를 맞히도록 학습된다.

##### 학습 과정

1. 초기 모델 설정: 데이터의 평균값과 같은 간단한 상수로 초기 예측 $\hat{F}_0(x)$를 설정한다.
2. 잔차/음의 기울기 계산: 현재 모델 $\hat{F}_k(x)$와 실제값 $y$의 차이 $y - \hat{F}_k(x)$를 계산한다.  
   (회귀에서는 이를 잔차로 볼 수 있고, 분류에서는 손실 함수의 음의 기울기를 사용하는 방식으로 일반화된다.)
3. 약한 학습기 학습: 잔차(또는 음의 기울기)를 목표값으로 두고 약한 학습기(보통 트리) $h_k(x)$를 학습한다.
4. 모델 업데이트: 학습률(learning rate) $\alpha$로 업데이트 폭을 조절하며
   $$
   \hat{F}_{k+1}(x) = \hat{F}_k(x) + \alpha \cdot h_k(x)
   $$
   로 모델을 갱신한다.
5. 반복: 정해진 반복 횟수(혹은 수렴 조건)까지 반복한다.

##### 핵심 아이디어

GBM은 함수 공간에서 경사 하강법을 적용하는 것으로 해석할 수 있다.  
손실을 줄이는 방향(잔차/기울기)을 따라 약한 학습기를 하나씩 추가하면서 예측을 점진적으로 보정해 나간다.

##### GBM 주요 Hyperparameter

- Weak learners 개수 (n_estimators): 생성하는 트리의 개수를 결정한다. 수가 클수록 일반적으로 예측 성능이 개선되지만, 학습 속도가 저하될 수 있고 과적합 위험이 증가한다.
- 학습률 (Learning rate): 각 약한 학습기가 이전 모델의 잔차를 얼마나 빠르게 보정할지(반영할지) 결정하는 값이다. 값이 작으면 학습 속도는 느리지만 더 안정적이고 일반화 성능이 좋은 모델을 얻을 수 있다.
- 최대 깊이 (max_depth): 개별 의사결정나무(약한 학습기)의 최대 깊이를 제한한다. 깊이가 깊어지면 모델이 복잡해져 과적합될 가능성이 커진다.
- 변수 개수 (max_features): 개별 트리를 분할할 때 무작위로 사용하는 변수의 개수를 의미한다. 값이 클수록 트리 간 상관관계가 증가하여 랜덤 서브스페이스 효과가 감소한다.
- 최소 선택 관측치 수 (min_samples_leaf): 리프 노드가 되기 위한 최소 관측치 개수이다. 이 값이 크면 모델이 단순해지고 학습 시간은 감소하지만, 언더피팅(underfitting)될 수 있다.
- 최소 분리 샘플 수 (min_samples_split): 노드를 분할하기 위한 최소 관측치 개수이다. 이 값이 크면 과적합을 방지하는 데 도움이 된다.

## 고급 부스팅 알고리즘: XGBoost, LightGBM, CatBoost

XGBoost, LightGBM, CatBoost는 GBM의 성능을 유지하면서도 과적합 제어, 학습 속도 개선, 범주형 변수 처리 등 실무적 요구를 반영해 확장한 고급 부스팅 알고리즘들이다.  
데이터 과학 경진대회와 실무 환경에서 널리 활용된다.

### eXtreme Gradient Boosting (XGBoost)

XGBoost는 GBM에 정규화(Regularization)와 병렬 처리(Parallelism) 등을 적극적으로 적용하여  
과적합을 제어하고 학습 효율을 개선한 알고리즘이다.

#### 주요 특징

1. 일반화 성능 강화 (More regularized model formalization)
   - XGBoost는 목적 함수에 정규화 항(Regularization term)을 명시적으로 포함하여 과적합을 효과적으로 제어한다. 이는 개별 트리의 복잡도를 제한하고 모델 파라미터의 L1/L2 정규화를 통해 모델의 일반화 능력을 향상시킨다.
   - 트리 생성 시 가지치기(pruning) 기회를 늘리고, Similarity(잔차 제곱합 / 데이터 개수 + $\lambda$)와 Gain 계산에 정규화를 반영한다.  
     `If Gain < γ, then prune` 규칙을 사용하여 과도하게 깊은 트리 생성을 방지한다.
2. 학습 효율 향상 (Better support for multicore processing)
   - Column block for parallel learning: 데이터의 '블록(block)'이라는 인메모리 유닛을 활용해  
     데이터 정렬(sorting) 연산을 최소화하고, 블록 단위로 데이터를 재사용해 병렬 처리 효율을 높인다.  
     이를 통해 여러 CPU 코어를 활용한 병렬 학습이 가능하다.
   - Cache-aware block structure: CPU 캐시 미스(Cache Miss)로 인한 성능 저하를 줄이기 위해  
     스레드(thread)마다 내부 버퍼(internal buffer)를 할당하는 구조를 사용한다.  
     특히 탐욕적 탐색(greedy search) 과정에서 메모리 접근 효율을 높여 학습 속도를 개선한다.
   - Approximate Split Finding: 대규모 데이터셋의 경우, 모든 분할 후보 지점을 정확하게 계산하는 대신 근사적인 분할 지점을 찾아 학습 속도를 빠르게 한다.
3. 결측값 처리 기능: XGBoost는 자체적으로 결측값을 처리하는 기능을 가지고 있어, 사용자가 별도로 결측값을 대치할 필요가 없다.
4. 다양한 손실 함수 지원: 회귀, 분류 등 다양한 유형의 문제에 적용할 수 있도록 여러 손실 함수를 지원한다.

#### XGBoost 주요 Hyperparameter

GBM의 하이퍼파라미터와 유사하지만, 정규화 관련 파라미터가 추가된다.

- n_estimators: 생성하는 트리의 개수.
- learning_rate (eta): 학습률.
- max_depth: 개별 트리의 최대 깊이.
- subsample: 각 트리를 학습할 때 사용할 데이터 샘플링 비율 (Bagging과 유사).
- colsample_bytree: 각 트리를 학습할 때 사용할 피처(변수) 샘플링 비율 (Random Forest와 유사).
- gamma (min_split_loss): 리프 노드에 추가적인 분할을 생성하는 데 필요한 최소 손실 감소. 이 값보다 작으면 분할하지 않는다. (정규화)
- lambda (reg_lambda): L2 정규화 계수. (정규화)
- alpha (reg_alpha): L1 정규화 계수. (정규화)

### LightGBM

LightGBM은 Microsoft에서 개발한 부스팅 알고리즘으로,  
XGBoost 계열의 성능을 유지하면서 학습 속도와 메모리 효율을 크게 향상시키는 데 초점을 맞추었다.  
대규모 데이터에서 특히 강점을 보인다.

#### 핵심 특징

1. Gradient-based One-sided Sampling (GOSS)
   - 기존 부스팅 알고리즘은 모든 데이터 인스턴스를 사용하여 기울기(gradient)를 계산하고 학습을 진행한다. 하지만 이는 효율적이지 않다.
   - GOSS는 '큰 기울기(Large gradient)'를 갖는 관측치들(즉, 오분류되거나 예측 오차가 큰 중요한 데이터)은 모두 유지하고, '작은 기울기(Small gradient)'를 갖는 관측치들은 '일정 확률에 의해 임의로 제거'하면서 샘플링하여 학습에 사용한다.
   - 이를 통해 학습 속도를 크게 향상시키면서도, 모델의 정확도 손실을 최소화한다. 작은 기울기를 가진 데이터들도 완전히 버리지 않고 일부를 활용하여 데이터 분포의 정보 손실을 줄인다.
2. Exclusive Feature Bundling (EFB)
   - 희소(Sparse) 데이터셋, 즉 0이 매우 많은 데이터셋에서 많은 피처(변수)들이 동시에 0이 아닌 값을 가질 확률이 낮은 경우가 많다.
   - EFB는 이러한 피처들 중에서 '상호 배타적(mutually exclusive)'인(동시에 0이 아닌 값을 가지는 경우가 드문) 피처들을 '묶음(bundling)'으로 처리하여 새로운 하나의 피처로 만든다.
     - 예를 들어 One-Hot Encoding으로 생성된 피처들은 대부분 상호 배타적이다.
       EFB는 이러한 피처 번들링으로 피처 수를 줄여 학습 속도와 메모리 효율을 크게 개선한다.  
       이 과정은 '그래프 색칠하기(Graph Coloring)' 알고리즘과 유사하게 동작한다.
3. Leaf-wise (vs. Level-wise) 트리 성장 방식
   - XGBoost와 같은 대부분의 의사결정나무 기반 부스팅 알고리즘은 'Level-wise' 방식으로 트리를 성장시킨다. 즉, 트리의 깊이를 한 단계씩 넓혀가며 모든 노드를 동일하게 분할한다.
   - LightGBM은 'Leaf-wise' (또는 'Best-first') 방식으로 트리를 성장시킨다. 이는 전체 트리에서 가장 큰 손실 감소(Gain)를 가져오는 리프 노드를 우선적으로 분할한다.
   - 장점: 동일한 수의 노드를 생성할 경우, Leaf-wise 방식이 Level-wise 방식보다 더 복잡하고 비대칭적인 트리를 생성하여 예측 오류를 더 효율적으로 줄일 수 있다.
   - 단점: 특정 분기에서 과도하게 깊은 트리가 생성되어 과적합에 더 취약할 수 있으므로, 'max_depth'와 같은 하이퍼파라미터로 깊이를 적절히 제어하는 것이 중요하다.

### CatBoost

CatBoost는 Categorical Boosting의 약자로,  
특히 범주형 변수(categorical features)를 다루는 방식과 예측 이동(prediction shift) 문제를 줄이는 데 초점을 둔 부스팅 알고리즘이다.  
Yandex에서 개발했다.

#### CatBoost가 초점을 맞춘 문제 상황

1. 종속변수를 활용한 범주형 변수 변환의 문제점 (Target Leakage)
   - 범주형 변수를 Target Encoding처럼 숫자로 바꿀 때, 범주별 평균 종속변수 값을 쓰는 경우가 많았다.  
     이 과정에서 테스트 시점에는 알 수 없는 $i$번째 관측치의 $y_i$ 정보가 간접적으로 반영되는 문제가 발생할 수 있으며,  
     이는 데이터 누수(data leakage)로 이어질 수 있다.
   - CatBoost는 이를 해결하기 위해 'Ordered Target Statistics (Ordered TS)'라는 기법을 사용한다.
     - Ordered TS: 관측치에 시계열 정보를 부여해, 범주형 변수를 인코딩할 때  
       '현재 시점 이전에 관측된' 데이터의 종속변수 값만 사용해 평균을 계산한다.  
       이를 통해 미래 정보 사용을 방지하고 데이터 누수를 줄인다.
2. 학습 및 예측 데이터 간 차이 존재의 문제점 (Prediction Shift Problem)
   - 훈련 데이터와 실제 서비스 환경(예측 데이터) 간 데이터 분포가 다를 때,  
     모델의 예측 성능이 떨어지는 현상을 '예측 이동(Prediction Shift)'이라고 한다.  
     부스팅은 순차 학습이며 이전 모델의 잔차에 의존하는 구조이므로 이러한 문제가 발생할 수 있다.
   - CatBoost는 'Ordered Boosting'이라는 기법을 사용하여 이를 완화한다.
     - Ordered Boosting: 각 반복에서 새로운 약한 학습기를 학습할 때 무작위 순열(random permutation)로  
       훈련 데이터를 재정렬하고, 그 순열에 따라 Ordered TS를 적용해 예측 이동 문제를 줄인다.  
       이를 통해 훈련 데이터에 과도하게 적합되는 것을 방지하고 일반화 성능을 높인다.

#### 범주형 변수 처리

CatBoost는 별도의 전처리 없이 범주형 변수를 직접 입력으로 받아들인다.  
내부적으로 One-Hot Encoding, Ordered TS 등 통계적 인코딩 기법을 조합해  
범주형 변수를 숫자형으로 변환해 사용한다. (다른 부스팅에서는 보통 사용자가 수동으로 처리)

#### Balanced Tree 성장 방식

XGBoost의 Level-wise, LightGBM의 Leaf-wise와 달리 CatBoost는 'Balanced Tree' 방식으로 트리를 성장시킨다.  
Level-wise처럼 깊이를 한 단계씩 확장하되, 각 분기에서 모든 노드의 깊이가 같도록 균형을 맞춘다.  
이를 통해 모델의 안정성을 높이고 과적합을 제어한다.
